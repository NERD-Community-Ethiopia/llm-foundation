# Attention is All You Need - Workshop

This workshop guides students through implementing the Transformer architecture from scratch, building a Bible translation model from Amharic to Oromiffa.

## Checkpoints

### Checkpoint 0: Environment Setup âœ…
- [x] Project structure
- [x] Dependencies installation
- [x] Git repository setup

### Checkpoint 1: Neural Networks & Backpropagation
- [ ] Basic feedforward neural network
- [ ] Backpropagation implementation
- [ ] Simple training examples

### Checkpoint 2: RNN/LSTM Implementation
- [ ] RNN from scratch
- [ ] LSTM from scratch
- [ ] Sequence processing examples

### Checkpoint 3: Attention Mechanism
- [ ] Basic attention implementation
- [ ] Attention visualization
- [ ] Sequence-to-sequence with attention

### Checkpoint 4: Transformer Core Components
- [ ] Multi-Head Self-Attention
- [ ] Positional Encoding
- [ ] Feed-Forward Networks
- [ ] Layer Normalization

### Checkpoint 5: Complete Transformer
- [ ] Encoder-Decoder architecture
- [ ] Multiple transformer blocks
- [ ] Masking implementation

### Checkpoint 6: Data Preparation
- [ ] Bible dataset processing
- [ ] Tokenization implementation
- [ ] Data pipeline setup

### Checkpoint 7: Training Infrastructure
- [ ] Training loop
- [ ] Teacher forcing
- [ ] Learning rate scheduling

### Checkpoint 8: Model Training
- [ ] Full training pipeline
- [ ] Beam search inference
- [ ] Evaluation metrics

### Checkpoint 9: FastAPI Backend
- [ ] REST API endpoints
- [ ] Model serving
- [ ] API documentation

### Checkpoint 10: Gradio Frontend
- [ ] Web interface
- [ ] Real-time translation
- [ ] User experience features

### Checkpoint 11: Deployment
- [ ] Docker containerization
- [ ] Production deployment
- [ ] Performance monitoring

## Getting Started

1. Clone this repository
2. Install dependencies: `pip install -r requirements.txt`
3. Follow the checkpoints in order
4. Each checkpoint has its own directory with implementation and tests

## Dataset

We'll be using Bible translations in Amharic and Oromiffa for training our translation model. 