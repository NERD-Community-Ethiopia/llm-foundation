#!/usr/bin/env python3
"""
Real Data Testing Runner - Fixed Implementation
"""
import sys
import os
import numpy as np
import matplotlib.pyplot as plt

# Add src to path
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

def test_attention_with_real_data():
    """Test attention mechanisms with real text data"""
    print("üöÄ REAL DATA ATTENTION TESTING")
    print("=" * 80)
    
    # Sample real datasets
    sentences = [
        "The cat sat on the mat",
        "I love attention mechanisms", 
        "Transformers revolutionized NLP",
        "The quick brown fox jumps over the lazy dog",
        "Machine learning is fascinating"
    ]
    
    translation_pairs = [
        ("Hello world", "Bonjour le monde"),
        ("I love you", "Je t'aime"),
        ("Good morning", "Bonjour"),
        ("Thank you", "Merci"),
        ("How are you", "Comment allez-vous")
    ]
    
    # Create vocabulary
    all_words = set()
    for sentence in sentences:
        all_words.update(sentence.lower().split())
    for eng, fr in translation_pairs:
        all_words.update(eng.lower().split())
        all_words.update(fr.lower().split())
    
    all_words.update(['<PAD>', '<UNK>', '<START>', '<END>'])
    vocab = {word: idx for idx, word in enumerate(sorted(all_words))}
    vocab_size = len(vocab)
    
    print(f" Vocabulary size: {vocab_size}")
    print(f"üìù Number of sentences: {len(sentences)}")
    print(f"üåç Number of translation pairs: {len(translation_pairs)}")
    
    # Test 1: Basic Attention with Translation
    print("\nüåç TEST 1: Basic Attention with Translation")
    print("-" * 50)
    
    try:
        from attention.basic_attention.attention import BasicAttention
        
        # Create attention mechanism
        attention = BasicAttention(query_dim=vocab_size, key_dim=vocab_size, value_dim=vocab_size)
        
        for eng_text, fr_text in translation_pairs[:2]:
            print(f"\n Translation: '{eng_text}' ‚Üí '{fr_text}'")
            
            # Convert to embeddings (simplified)
            eng_words = eng_text.lower().split()
            fr_words = fr_text.lower().split()
            
            # Create simple embeddings
            eng_embeddings = np.random.randn(len(eng_words), vocab_size) * 0.1
            fr_embeddings = np.random.randn(len(fr_words), vocab_size) * 0.1
            
            # Apply attention
            output, attention_weights = attention.forward(eng_embeddings, fr_embeddings, fr_embeddings)
            
            print(f"   English words: {eng_words}")
            print(f"   French words: {fr_words}")
            print(f"   Attention weights shape: {attention_weights.shape}")
            
            # Visualize attention
            visualize_attention_weights(attention_weights, eng_words, fr_words, 
                                      f"Translation: {eng_text} ‚Üí {fr_text}")
        
    except ImportError as e:
        print(f"‚ùå Could not import BasicAttention: {e}")
    
    # Test 2: Self-Attention with Sentences (FIXED)
    print("\nüß† TEST 2: Self-Attention with Sentences")
    print("-" * 50)
    
    try:
        from attention.self_attention.self_attention import SelfAttention
        
        # FIX: Ensure input_dim is divisible by num_heads
        # Use a smaller embedding dimension that's divisible by 2
        embedding_dim = 32  # 32 is divisible by 2 (num_heads)
        
        # Create self-attention mechanism with fixed dimensions
        self_attn = SelfAttention(input_dim=embedding_dim, num_heads=2)
        
        for sentence in sentences[:2]:
            print(f"\nüìù Sentence: '{sentence}'")
            
            words = sentence.split()
            # Use fixed embedding dimension instead of vocab_size
            embeddings = np.random.randn(len(words), embedding_dim) * 0.1
            
            # Apply self-attention
            output, attention_weights = self_attn.forward(embeddings)
            
            print(f"   Words: {words}")
            print(f"   Embedding dimension: {embedding_dim}")
            print(f"   Attention weights shape: {attention_weights.shape}")
            
            # Visualize self-attention
            visualize_attention_weights(attention_weights, words, words,
                                      f"Self-Attention: {sentence}")
        
    except ImportError as e:
        print(f"‚ùå Could not import SelfAttention: {e}")
    except Exception as e:
        print(f"‚ùå Self-attention error: {e}")
    
    # Test 3: Multi-Head Attention (FIXED)
    print("\n TEST 3: Multi-Head Attention")
    print("-" * 50)
    
    try:
        from attention.multi_head_attention.multi_head_attention import MultiHeadAttention
        
        # FIX: Use embedding dimension that's divisible by num_heads
        embedding_dim = 32  # 32 is divisible by 2 (num_heads)
        
        # Create multi-head attention
        mha = MultiHeadAttention(d_model=embedding_dim, num_heads=2)
        
        # Test with a sentence
        sentence = "The transformer model revolutionized"
        words = sentence.split()
        embeddings = np.random.randn(1, len(words), embedding_dim) * 0.1  # Add batch dimension
        
        # Apply multi-head attention
        output, attention_weights = mha.forward(embeddings, embeddings, embeddings)
        
        print(f"   Sentence: '{sentence}'")
        print(f"   Words: {words}")
        print(f"   Embedding dimension: {embedding_dim}")
        print(f"   Output shape: {output.shape}")
        print(f"   Attention weights shape: {attention_weights.shape}")
        
        # Visualize multi-head attention
        visualize_multi_head_attention(attention_weights[0], words, "Multi-Head Attention")
        
    except ImportError as e:
        print(f"‚ùå Could not import MultiHeadAttention: {e}")
    except Exception as e:
        print(f"‚ùå Multi-head attention error: {e}")
    
    print("\n" + "=" * 80)
    print("‚úÖ REAL DATA ATTENTION TESTING COMPLETED!")
    print("üìä Check the generated visualizations to see attention in action!")

def visualize_attention_weights(weights, query_words, key_words, title):
    """Visualize attention weights"""
    plt.figure(figsize=(10, 8))
    
    # Truncate to actual word lengths
    actual_query_len = len(query_words)
    actual_key_len = len(key_words)
    weights_actual = weights[:actual_query_len, :actual_key_len]
    
    im = plt.imshow(weights_actual, cmap='Blues', aspect='auto')
    plt.colorbar(im)
    
    # Set labels
    plt.xticks(range(actual_key_len), key_words, rotation=45, ha='right')
    plt.yticks(range(actual_query_len), query_words)
    
    plt.title(title, fontsize=14, weight='bold')
    plt.xlabel('Key Words')
    plt.ylabel('Query Words')
    
    # Add text annotations
    for i in range(actual_query_len):
        for j in range(actual_key_len):
            plt.text(j, i, f'{weights_actual[i, j]:.2f}', 
                    ha='center', va='center', fontsize=8)
    
    plt.tight_layout()
    filename = f'attention_real_data_{title.lower().replace(" ", "_").replace(":", "").replace("‚Üí", "to")}.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"   üìä Visualization saved: {filename}")

def visualize_multi_head_attention(weights, words, title):
    """Visualize multi-head attention"""
    num_heads = weights.shape[0]
    fig, axes = plt.subplots(1, min(num_heads, 2), figsize=(15, 6))
    fig.suptitle(f'{title}: {num_heads} Heads', fontsize=16, weight='bold')
    
    if num_heads == 1:
        axes = [axes]
    
    for head in range(min(num_heads, 2)):
        ax = axes[head]
        head_weights = weights[head]
        
        # Truncate to actual word length
        actual_len = len(words)
        head_weights_actual = head_weights[:actual_len, :actual_len]
        
        im = ax.imshow(head_weights_actual, cmap='viridis', aspect='auto')
        ax.set_title(f'Head {head + 1}')
        ax.set_xticks(range(actual_len))
        ax.set_xticklabels(words, rotation=45, ha='right')
        ax.set_yticks(range(actual_len))
        ax.set_yticklabels(words)
        
        # Add colorbar
        plt.colorbar(im, ax=ax)
    
    plt.tight_layout()
    plt.savefig('multi_head_attention_real_data.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"   üìä Multi-head visualization saved: multi_head_attention_real_data.png")

def test_text_classification():
    """Test attention-based text classification"""
    print("\nüìö TEXT CLASSIFICATION WITH ATTENTION")
    print("=" * 60)
    
    # Sample data
    texts = [
        "I love this movie it was amazing",
        "This film was terrible and boring", 
        "Great acting and wonderful story",
        "Waste of time horrible film",
        "Excellent performance by the actors",
        "Boring plot and bad acting"
    ]
    
    labels = [1, 0, 1, 0, 1, 0]  # 1 = positive, 0 = negative
    
    # Create vocabulary
    all_words = set()
    for text in texts:
        all_words.update(text.lower().split())
    
    vocab = {word: idx for idx, word in enumerate(sorted(all_words))}
    vocab_size = len(vocab)
    
    print(f"Vocabulary size: {vocab_size}")
    print(f"Number of classes: 2 (positive/negative)")
    
    # Simple attention-based classifier
    class SimpleAttentionClassifier:
        def __init__(self, vocab_size, num_classes, hidden_dim=32):
            self.vocab_size = vocab_size
            self.num_classes = num_classes
            self.hidden_dim = hidden_dim
            
            # Initialize weights
            self.embedding_dim = 16
            self.embeddings = np.random.randn(vocab_size, self.embedding_dim) * 0.01
            
            # Attention weights
            self.W_q = np.random.randn(self.embedding_dim, hidden_dim) * 0.01
            self.W_k = np.random.randn(self.embedding_dim, hidden_dim) * 0.01
            self.W_v = np.random.randn(self.embedding_dim, hidden_dim) * 0.01
            
            # Classification weights
            self.W_classifier = np.random.randn(hidden_dim, num_classes) * 0.01
            self.b_classifier = np.zeros((num_classes,))
        
        def forward(self, text_indices):
            # Get embeddings
            embeddings = self.embeddings[text_indices]
            
            # Apply attention
            Q = np.dot(embeddings, self.W_q)
            K = np.dot(embeddings, self.W_k)
            V = np.dot(embeddings, self.W_v)
            
            # Compute attention scores
            scores = np.dot(Q, K.T) / np.sqrt(self.hidden_dim)
            attention_weights = self.softmax(scores)
            
            # Apply attention to values
            attended = np.dot(attention_weights, V)
            
            # Global average pooling
            pooled = np.mean(attended, axis=0)
            
            # Classification
            logits = np.dot(pooled, self.W_classifier) + self.b_classifier
            predictions = self.softmax(logits)
            
            return predictions, attention_weights
        
        def softmax(self, x):
            exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
            return exp_x / np.sum(exp_x, axis=-1, keepdims=True)
    
    # Create classifier
    classifier = SimpleAttentionClassifier(vocab_size=vocab_size, num_classes=2)
    
    # Test on each text
    for i, (text, label) in enumerate(zip(texts, labels)):
        print(f"\n Text {i+1}: '{text}'")
        print(f"   True label: {'Positive' if label == 1 else 'Negative'}")
        
        # Convert text to indices
        words = text.lower().split()
        indices = [vocab.get(word, 0) for word in words]
        
        # Get predictions and attention
        predictions, attention_weights = classifier.forward(indices)
        
        predicted_label = np.argmax(predictions)
        confidence = np.max(predictions)
        
        print(f"   Predicted: {'Positive' if predicted_label == 1 else 'Negative'} (confidence: {confidence:.3f})")
        print(f"   Attention weights shape: {attention_weights.shape}")
        
        # Visualize attention
        plt.figure(figsize=(8, 6))
        im = plt.imshow(attention_weights, cmap='Blues', aspect='auto')
        plt.colorbar(im)
        
        plt.xticks(range(len(words)), words, rotation=45, ha='right')
        plt.yticks(range(len(words)), words)
        plt.title(f'Attention Weights: "{text}"')
        plt.xlabel('Words')
        plt.ylabel('Words')
        
        plt.tight_layout()
        plt.savefig(f'attention_classification_text_{i+1}.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"   üìä Visualization saved: attention_classification_text_{i+1}.png")

def main():
    print("üöÄ Running Real Data Attention Tests\n")
    
    try:
        # Run real data tests
        test_attention_with_real_data()
        
        # Run text classification test
        test_text_classification()
        
        print("\n" + "=" * 80)
        print("‚úÖ ALL REAL DATA TESTS COMPLETED SUCCESSFULLY!")
        print("üìä Check the generated visualizations to see attention in action!")
        print("üéØ This demonstrates attention mechanisms with actual text data!")
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 